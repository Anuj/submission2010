\section{Study}

We designed and implemented three different mechanisms called
frontside QWERTY (or QWERTY/soft-QWERTY), backside QWERTY and chording
mechanism. We discussed the details of the mechanisms in the last
section. In this section we describe the experimental process we used
to evaluate our designs.

\subsection{Participants}

\todo{This seems like rationalization.  We chose the sample out of convenience.  Lets be honest about it, and then discuss the strenghts and weaknesses of the sample set.}

For the purpose of the study we tried to keep the backgrounds of the
participants mixed, but also ensured that all of them have decent
exposure to typing on QWERTY keyboards (not necessarily
soft-QWERTY). The participants in the study were all working
professionals with more than 3 years of experience (on average) with
QWERTY keyboards. It should be noted that all the participants in our
experiment were part of a research environment and had reasonably high
technical backgrounds. Moreover, we had a total of 36 participants in
our study and the selection was not perfectly random. However, in
spite of these limitations, working with this audience was important
because our aim was to design for users who use soft-QWERTY keyboards
on a regular basis. Our belief was strengthened by the finding in
post-test interviews, that out of the total of 36 participants in the
study, 32 participants used soft-QWERTY keyboards on a daily basis and
the rest of the participants had reasonably high prior exposure, but
did not own a device with a soft-QWERTY keyboard. This information was
important as familiarity with a particular style of text-input
mechanism gets reflected in the kind of speed people achieve with the
same input mechanism. Though we tried to achieve a reasonable mix of
backgrounds for participants, we acknowledge the fact that we still
had a sample that was experienced. Therefore, our results are
representative of participants who have had reasonable level of
exposure to typing on QWERTY keyboards. For novices, the results might
be different, in any direction.

\subsection{Phase 1: Initial test}

After we implemented the QWERTY, backside QWERTY and chording
mechanism, we conducted some initial tests to get feedback. We
conducted the first onewith 3 participants, and the
second one with 6 participants. For the purpose of the initial tests
we picked a text corpus that was different from the corpus used in
phase 2. Both corpora were generated by choosing randomly from Scott
Mackenzie's text corpus [17], and were mutually exclusive. The
statistics for the two are listed in Table 1.

\begin{table}
	\centering
		\begin{tabular}{rcc}
		                         & \begin{minipage}{2cm} \centering \color{grey}{Study corpus}\end{minipage} & \begin{minipage}{2cm} \centering \color{grey}{Initial test corpus}\end{minipage}  \\ 
			 \color{grey}{Average phrase length} & 15.07 & 15.67 \\ 
			 \color{grey}{Number of words} & 76 & 81 \\ 
			 \color{grey}{Unique words} & 62 & 67 \\ 
			 \color{grey}{Min. length of word} & 1 & 1 \\ 
			 \color{grey}{Max. length of word} & 11 & 12 \\ 
			 \color{grey}{Average word length} & 4.95 & 4.43 \\ 
			 \color{grey}{Number of characters} & 437 & 425 \\ 
			 \color{grey}{Correlation with English} & 0.9297 & 0.9377 \\ 
		\end{tabular}
	\caption{Statistics for text corpora}
	\label{tab:StatisticsForTextCorpora}
\end{table}

The participants in the initial test spent 20 minutes familiarizing
themselves with the interface and were then asked to enter the entire
initial test corpus. The entire process was captured on video and
post-test interviews were also conducted. Based on the post-test
interviews and analysis of videos, we made usability refinements to
the interfaces.

\subsection{Phase 2: Usability Study}
\subsubsection{Conditions}

The experiment had three conditions. As mentioned earlier, there were
a total of 36 participants. Each condition had 12 participants. The
three conditions were: frontside QWERTY (or QWERTY), Backside QWERTY
and Chording Mechanism.

\subsubsection{Data collection methods}

To make sure that we captured all the important aspects of the
experiment and the feedback from the users, we used multiple data
collection methods:

\begin{itemize}
\item Videos: We recorded every sessions fully. In total, we recorded
  around 20 hours of videos.
\item Data Logs: All the mechanisms had a built in data logging
  feature, that recorded each action by the user, along with
  timestamps. This helped in understanding parts of the videos, where
  the user was stuck or had trouble accomplishing what they wanted.
\item Post-session interviews: After each session, we asked the
  participants to report on their experience with the interface. We
  did this to get feedback on the mechanisms.  The results from
  these interviews are reflected in the future work section of the
  paper.
\item NASA task load index: In order to quantitatively capture the
  experience with the interfaces, we used the NASA task load
  index. The NASA-TLX is a widely used tool used to report perceived
  workload assessment. It is divided into six different scales: Mental
  Demand, Physical Demand, Temporal Demand, Performance, Effort, and
  Frustration. It was developed at NASA's Ames Research Center over
  three years and 40 lab simulations [18]. Since it is a widely used
  method for quantifying user perception of task difficulty and the
  demand on the performer of the task, it was well suited for our
  work. We wanted to explore the use of back-of-device interactions
  for text-input, but more than that we wanted to know if uses see
  such interactions as frustrating. The TLX allowed us to do exactly
  that.

  It should be noted that the NASA TLX scale has 20 divisions, each
  division corresponding to 5 task load points (making it a 100 point
  scale).

\end{itemize}


\subsubsection{Measures}
\begin{itemize}
\item Keystrokes Per Character (KSPC): KSPC is generally treated as a
  measure of accuracy, because it represents the number of keystrokes
  executed per character.  Note that we regard a keystroke in the
  chording mechansim as a character entry, rather than a screen touch.
\item Words Per Minute (WPM): Words Per Minute (WPM) is a measure that
  is commonly used to represent the speed of a text input mechanism.
\item Speed vs Accuracy trade-off: An efficient mechanism should
  achieve a trade-off between speed and accuracy. This measure studies
  the two measures together.
\end{itemize}

\subsubsection{Process}

The participants in the study went through the following steps during
the study. Care was taken that the steps remained the same across all
participants so as to control the environment. This reduced the
possibility of the environment acting as a confounding variable in the
experiment. 

\begin{enumerate}
\item We briefed the participant about the goal of the session. We also
  briefed them about the structure of the session.
\item One of the researchers gave a brief introduction regarding to
  the input mechanism. 
\item We gave the participant a practice corpus with which to spend
  the next 20 mins familiarizing themselves with the input mechanism.
\item After 20 minutes, we gave the participant the study corpus, which
  was different from the corpus given during familiarization, so that
  they are not familiar with the text they were supposed to
  enter. They were asked to input the corpus in its entirety, using
  the mechanism that they had just encountered. They were asked to be
  accurate with their input, and the system would underline their
  mistakes as and when they occured.
\item Once the participants had entered the entire text without any
  errors, they were handed the NASA TLX questionnaire and asked to
  rate their experience.
\item Finally, they were interviewed on any other qualitative feedback
  they had on how to make the mechanisms better.
\item The entire process was videotaped for data analysis and
  validation purposes.
\end{enumerate}
	