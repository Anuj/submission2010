\section{Experiment}

We designed three different mechanisms called frontside QWERTY (or QWERTY/soft-QWERTY), backside QWERTY and chording mechanism, as the solutions to the "stationary and visually focused", "mobile and visually focused" and "mobile and intermittently focused" scenarios respectively. The details of the mechanisms will be described in the next section. In this sections we describe the environment in which the mechanisms were incrementally developed and also the process that was employed. It is important to understand the structure of the study, before we try to explain the incremental changes in the design of the mechanisms.

\subsection{Participants}

For the purpose of the study we tried to keep the backgrounds of the
participants mixed, but also ensured that all of them have decent
exposure to typing on QWERTY keyboards (not necessarily
soft-QWERTY). The participants in the study were all working
professionals with more than 3 years of experience (on an average)
with QWERTY keyboards. According to post-test interviews, out of the
total of 36 participants in the study, 32 participants had prior
experience with soft-QWERTY keyboards. This information was important
as familiarity with a particular style of text-input mechanism gets
reflected in the kind of speed people achieve with the same. Though we
tried to achieve a reasonable mix of backgrounds for participants, we
acknowledge the fact that we still had a sample that was
experienced. Therefore, our results are representative of participants
who have had reasonable level of exposure to typing on QWERTY
keyboards. For novices, the results might be different, in any
direction.

\subsection{Phase 1: Usability test}

After we implemented the QWERTY, backside QWERTY and chording
mechanism, we conducted two rounds of usability evaluation. The first
one with 3 participants, and the second one with 6 participants. For
the purpose of the usability study we picked a text corpus that was
different from the corpus used in the phase 2. Both the corpora were
generated by choosing randomly from Scott Mackenzie's text corpus
[Reference], and were mutually exclusive. The statistics for the two
are listed in Table 1.

\begin{table*}
	\centering
		\begin{tabular}{|l|c|c|} \hline
		                         & Experiment corpus & Usability test corpus \\ \hline
			 Average phrase length & 15.07 & 15.67 \\ \hline
			 Number of words & 76 & 81 \\ \hline
			 Unique words & 62 & 67 \\ \hline
			 Min. length of word & 1 & 1 \\ \hline
			 Max. length of word & 11 & 12 \\ \hline
			 Average word length & 4.95 & 4.43 \\ \hline
			 Number of characters & 437 & 425 \\ \hline
			 Correlation with English & 0.9297 & 0.9377 \\ \hline
		\end{tabular}
	\caption{Statistics for text corpora}
	\label{tab:StatisticsForTextCorpora}
\end{table*}

The participants in the usability test spent 20 minutes familiarizing
themselves with the interface and were then asked to enter the entire
usability test corpus. The entire process was captured on videos and
post-test interviews were also conducted. Based on the post-test
interviews and analysis of videos, changes to the interfaces were
made.

\subsection{Phase 2: Scientific experiment}
\subsubsection{Conditions}

The experiment had three conditions. As mentioned earlier, there were
a total of 36 participants. Each condition had 12 participants. The
three conditions were:

\begin{itemize}
	\item frontside QWERTY (or QWERTY)
	\item Backside QWERTY
	\item Chording Mechanism
\end{itemize}
\subsubsection{Data collection methods}

To make sure that all the important aspects of the experiment and the
feedback from the users is captured fully, we used multiple data
collection methods:

\begin{itemize}
\item Videos: All the sessions were fully recorded. In total, around
  20 hours of videos were recorded, by the end of the experiment.
\item Data Logs: All the mechanisms had a built in data logging
  feature, that recorded each and every action of the user, along with
  timestamps. This helped in understanding parts of the videos, where
  the user was stuck or had trouble accomplishing what they wanted.
\item Post-session interviews: After each session, the participants
  were asked to report on their experience with the interface. To give
  the discussion some structure, the users were asked the following
  questions:

\begin{enumerate}
	\item What did you like the most?
	\item What did you dislike the most?
	\item What would you change in the interface?
\end{enumerate}

This was done to get feedback on the mechanisms and the results from these interviews are reflected in the future work section of the paper. 

\item NASA task load index: To be able to quantitatively capture the
  experience with the interfaces, the NASA task load index was
  used. The NASA Task Load Index (NASA-TLX) is a subjective,
  multidimensional assessment tool that rates perceived workload on
  six different subscales: Mental Demand, Physical Demand, Temporal
  Demand, Performance, Effort, and Frustration. It was developed by
  the Human Performance Group at NASA's Ames Research Center over a
  three year development cycle that included more than 40 laboratory
  simulations [Reference] [Reference]. It has been cited in over 550
  studies[Reference]. These statistics highlight the
  large influence the NASA-TLX has had in Human Factors
  research. Therefore, we chose it as a tool in our experiment to
  capture the user experience. It should be noted that the NASA TLX scale has 20 divisions, each division corresponding to 5 task load points (making it a 100 point scale). Researchers tend to use both the scales, depending on the level of granularity needed. For clarity purposes, we will always state the scale we are using while making an argument henceforth. 

\end{itemize}
\subsubsection{Measures}
\begin{itemize}
	\item Keystrokes Per Character (KSPC)
	\item Words Per Minute (WPM)
	\item Speed vs Accuracy tradeoff 
\end{itemize}
\subsubsection{Process}

The participants in the study were supposed to go through the
following steps during the study. Care was taken that the steps remain
the same across all participants so as to control the
environment. Ideally, the mechanisms should have been tested out in
the scenarios that we had earlier listed. However, the lack of prior
research on the topic, suggested that the first few research cycles
should be conducted under controlled conditions. This eliminated the
possibility of the environment acting as a confounding variable in the
experiment. The process followed was:

\begin{enumerate}
\item Participants were briefed about the goal of the session. They
  were also briefed about the structure of the session.
\item They were given a brief introduction to the input
  mechanism. This was done by one of the researchers.
\item They were given the experiment corpus [Table 1] and asked to spend the next 20
  mins familiarizing themselves with the input mechanism.
\item The entire process was videotaped for data analysis and
  validation purposes.
\item After 20 minutes, they were handed over the experiment
  corpus. They were asked to input the corpus in its entirety, using
  the mechanism that they had just encountered. They were asked to be
  accurate with their input, and the system would underline their
  mistakes as and when they occur.
\item Once the participants had entered the entire text without any
  errors, they were handed the NASA TLX questionaire and asked to rate
  their experience. Since the index is relative they were asked to
  compare their experience with their previous exposure to a soft
  QWERTY keyboard. 
\item Finally, they were interviewed on any other qualitative feedback
  they had on how to make the mechanisms better.
\end{enumerate}
	