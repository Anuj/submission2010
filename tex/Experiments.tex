\section{Study}

We designed and implemented three different mechanisms called frontside QWERTY (or QWERTY/soft-QWERTY), backside QWERTY and chording mechanism. The details of the mechanisms were discussed in the last section. In this section we describe the environment in which the mechanisms were incrementally developed and also the process that was employed. 

\subsection{Participants}

For the purpose of the study we tried to keep the backgrounds of the participants mixed, but also ensured that all of them have decent exposure to typing on QWERTY keyboards (not necessarily soft-QWERTY). The participants in the study were all working professionals with more than 3 years of experience (on average) with QWERTY keyboards. It should be noted that all the participants in our experiment were part of a research environment and had reasonably high technical backgrounds. Moreover, we had a total of 36 participants in our study and the selection was not perfectly random. However, in spite of these limitations, working with this audience was important because our aim was to design for users who use soft-QWERTY keyboards on a regular basis. Our belief was strengthened by the finding in post-test interviews, that out of the total of 36 participants in the study, 32 participants used soft-QWERTY keyboards on a daily basis and the rest of the participants had reasonably high prior exposure, but did not own a device with a soft-QWERTY keyboard. This information was important as familiarity with a particular style of text-input mechanism gets reflected in the kind of speed people achieve with the same input mechanism. Though we tried to achieve a reasonable mix of backgrounds for participants, we acknowledge the fact that we still had a sample that was experienced. Therefore, our results are representative of participants who have had reasonable level of exposure to typing on QWERTY keyboards. For novices, the results might be different, in any direction.

\subsection{Phase 1: Initial test}

After we implemented the QWERTY, backside QWERTY and chording mechanism, some initial tests were conducted to get some feedback. The first one was conducted with 3 participants, and the second one with 6 participants. For the purpose of the initial tests we picked a text corpus that was different from the corpus used in phase 2. Both corpora were generated by choosing randomly from Scott Mackenzie's text corpus [17], and were mutually exclusive. The statistics for the two are listed in Table 1.

\begin{table}
	\centering
		\begin{tabular}{rcc}
		                         & \begin{minipage}{2cm} \centering \color{grey}{Study corpus}\end{minipage} & \begin{minipage}{2cm} \centering \color{grey}{Initial test corpus}\end{minipage}  \\ 
			 \color{grey}{Average phrase length} & 15.07 & 15.67 \\ 
			 \color{grey}{Number of words} & 76 & 81 \\ 
			 \color{grey}{Unique words} & 62 & 67 \\ 
			 \color{grey}{Min. length of word} & 1 & 1 \\ 
			 \color{grey}{Max. length of word} & 11 & 12 \\ 
			 \color{grey}{Average word length} & 4.95 & 4.43 \\ 
			 \color{grey}{Number of characters} & 437 & 425 \\ 
			 \color{grey}{Correlation with English} & 0.9297 & 0.9377 \\ 
		\end{tabular}
	\caption{Statistics for text corpora}
	\label{tab:StatisticsForTextCorpora}
\end{table}

The participants in the initial test spent 20 minutes familiarizing themselves with the interface and were then asked to enter the entire initial test corpus. The entire process was captured on video and post-test interviews were also conducted. Based on the post-test interviews and analysis of videos, changes to the interfaces were made to get the mechanisms ready for larger scale evaluation.

\subsection{Phase 2: Usability Study}
\subsubsection{Conditions}

The experiment had three conditions. As mentioned earlier, there were a total of 36 participants. Each condition had 12 participants. The three conditions were: frontside QWERTY (or QWERTY), Backside QWERTY and Chording Mechanism.

\subsubsection{Data collection methods}

To make sure that all the important aspects of the experiment and the feedback from the users is captured fully, we used multiple data collection methods:

\begin{itemize}
\item Videos: All the sessions were fully recorded. In total, around 20 hours of videos were recorded, by the end of the experiment.
\item Data Logs: All the mechanisms had a built in data logging feature, that recorded each and every action of the user, along with timestamps. This helped in understanding parts of the videos, where the user was stuck or had trouble accomplishing what they wanted.
\item Post-session interviews: After each session, the participants were asked to report on their experience with the interface. This was done to get feedback on the mechanisms and the results from these interviews are reflected in the future work section of the paper. 
\item NASA task load index: In order to quantitatively capture the experience with the interfaces, the NASA task load index was used. The NASA-TLX is widely used tool used to report perceived workload assessment. It is divided into six different scales: Mental Demand, Physical Demand, Temporal Demand, Performance, Effort, and Frustration. It was developed at NASA's Ames Research Center over three years and 40 lab simulations [18]. Since it is a widely used method for quantifying user perception of task difficulty and the demand on the performer of the task, it was ideally suited for our work. We wanted to explore the use of back-of-device interactions for text-input, but more than that we wanted to know if such interactions are seen as frustrating by the users. The TLX allowed us to do exactly that. 

It should be noted that the NASA TLX scale has 20 divisions, each division corresponding to 5 task load points (making it a 100 point scale).

\end{itemize}
\subsubsection{Measures}
\begin{itemize}
	\item Keystrokes Per Character (KSPC): KSPC is generally treated as a measure of accuracy, because it
represents the number of keystrokes executed per character.
	\item Words Per Minute (WPM): Words Per Minute (WPM) is a measure that is commonly used to represent
the speed of a text input mechanism.
	\item Speed vs Accuracy trade-off: An efficient mechanism should achieve a trade-off between speed and accuracy. This measure studies the two measures together.
\end{itemize}
\subsubsection{Process}

The participants in the study were supposed to go through the following steps during the study. Care was taken that the steps remained the same across all participants so as to control the environment. This eliminated the possibility of the environment acting as a confounding variable in the experiment. The process followed was:

\begin{enumerate}
\item Participants were briefed about the goal of the session. They were also briefed about the structure of the session.
\item They were given a brief introduction to the input mechanism. This was done by one of the researchers.
\item They were given the initial test corpus [Table 1] and asked to spend the next 20 mins familiarizing themselves with the input mechanism.
\item The entire process was videotaped for data analysis and validation purposes.
\item After 20 minutes, they were handed over the study corpus, which was different from the corpus given during familiarization, so that they are not familiar with the text they were supposed to enter. They were asked to input the corpus in its entirety, using the mechanism that they had just encountered. They were asked to be accurate with their input, and the system would underline their mistakes as and when they occured.
\item Once the participants had entered the entire text without any errors, they were handed the NASA TLX questionnaire and asked to rate their experience. 
\item Finally, they were interviewed on any other qualitative feedback they had on how to make the mechanisms better.
\end{enumerate}
	