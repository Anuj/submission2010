\section{Future Work}

This piece of research was one of the few recent efforts in the domain
of text-input mechanisms with backside touch input. There are many
directions that can serve as follow up to this work. Our post-session/post-test interviews helped us come up with some ideas on  ideas on this stream of research. However, the
future work can largely be organized into categories as follows:

\subsection{Haptic Feedback}

In the next phase, we would want to include haptics and tactile
feedback. This would entail producing vibrations and other cues to
signal a change on the interface. Especially in the case of the
chording mechanism, whenever the user switches from one zone to the
other, or selects a new segment, it should be coupled with some
feedback. This would also help the interface to be used by visually
challenged people as well.

\subsection{Proximity Sensing}

As of now, the users just see the position of the fingers on the
backside touch. Though the participants in our study were able to work
well with this setup, we feel that for wider acceptance the interface
should be able to track fingers with more robustness and greater
detail. We plan to use proximity sensors that would determine how far
the fingers are from the screen, and try to estimate the position of
the cursors even without touching the screen. This would allow for
faster positioning and orientation. We hope that this change would
help the users enter text even faster.

\subsection{Longitudinal deployments} 

The time that the users spent on the mechanisms was limited and
short. They had to get accustomed to the interface in 20 minutes and
then start on the test. In the future, for stronger results we would
want to run longitudinal deployments. This would give users enough
time to get used to the interface, and then our measurements would be
comparable to the more frequently occurring cases like QWERTY.

\subsection{Modified Chording}

To further minimize the amount of movement in the chording mechanism,
the future iterations of the mechanism would use the first touch to
select the zone, and the rest of the touched would select segments
irrespective of position. Therefore, the user won't be required to try
and bring 1-3 fingers into a particular zone. Instead they would just
take one finger to a particular zone and then place the others at any
random location thereby selecting the segments in the zone that was
selected by the first finger. This would considerably reduce the
effort involved in the chording mechanism, and would also make the
interface even more ready for "blind" text entry.

\subsection{Testing in valid scenarios}

In the introduction we talked about how there were some scenarios that we
envisioned and observed. However, since this was the first
evaluation of this sort we wanted to control as many variables as
possible. Therefore, very similar conditions were reproduced for all
the participants. In the near future we would want to test the
applications in various different scenarios, and analyse the appropriateness of different mechanisms in different scenarios.
